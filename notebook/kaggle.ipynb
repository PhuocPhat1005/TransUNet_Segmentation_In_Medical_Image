{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops import rearrange\n",
    "from scipy.ndimage import rotate, zoom\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Synapse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynapseDataset(Dataset):\n",
    "  def __init__(self, root_path, transform=None):\n",
    "    self.root_path = root_path\n",
    "    self.transform = transform\n",
    "    self.sample_list = os.listdir(root_path)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sample_list)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    slice_name = self.sample_list[idx].strip()\n",
    "    data_path = os.path.join(self.root_path, slice_name)\n",
    "    data = np.load(data_path)\n",
    "    image, mask = data['image'], data['label']\n",
    " \n",
    "    sample = {'image': image, 'mask': mask}\n",
    "    if self.transform: sample = self.transform(sample)\n",
    "    sample['case_name'] = self.sample_list[idx].strip()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGenerator:\n",
    "  def __random_rot_flip(self, image, mask):\n",
    "    k = np.random.randint(0, 4)\n",
    "    image = np.rot90(image, k)\n",
    "    mask = np.rot90(mask, k)\n",
    "    axis = np.random.randint(0, 2)\n",
    "    image = np.flip(image, axis=axis).copy()\n",
    "    mask = np.flip(mask, axis=axis).copy()\n",
    "    return image, mask\n",
    "  \n",
    "  def __random_rotate(self, image, mask):\n",
    "    angle = np.random.randint(-20, 20)\n",
    "    image = rotate(image, angle, order=0, reshape=False)\n",
    "    mask = rotate(mask, angle, order=0, reshape=False)\n",
    "    return image, mask\n",
    "\n",
    "  def __call__(self, sample):\n",
    "    image, mask = sample['image'], sample['mask']\n",
    "    rand = random.random()\n",
    "    if rand > 2/3:\n",
    "      image, mask = self.__random_rot_flip(image, mask)\n",
    "    elif rand > 1/3:\n",
    "      image, mask = self.__random_rotate(image, mask)\n",
    "    sample = {'image': image, 'mask': mask}\n",
    "    return sample\n",
    "  \n",
    "\n",
    "class Zoomer:\n",
    "  def __init__(self, output_size):\n",
    "    self.output_size = output_size\n",
    "  \n",
    "  def __zoom(self, image, mask):\n",
    "    x, y = image.shape\n",
    "    if x != self.output_size or y != self.output_size:\n",
    "      image = zoom(image, (self.output_size / x, self.output_size / y), order=3)\n",
    "      mask = zoom(mask, (self.output_size / x, self.output_size / y), order=0)\n",
    "    image = torch.from_numpy(image.astype(np.float32)).unsqueeze(0)\n",
    "    mask = torch.from_numpy(mask.astype(np.float32))\n",
    "    return image, mask\n",
    "  \n",
    "  def __call__(self, sample):\n",
    "    image, mask = sample['image'], sample['mask']\n",
    "    image, mask = self.__zoom(image, mask)\n",
    "    sample = {'image': image, 'mask': mask}\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "  'Synapse': SynapseDataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(DiceLoss, self).__init__()\n",
    "    self.n_classes = n_classes\n",
    "\n",
    "  def _one_hot_encoder(self, input_tensor):\n",
    "    tensor_list = []\n",
    "    for i in range(self.n_classes):\n",
    "      temp_prob = input_tensor == i\n",
    "      tensor_list.append(temp_prob.unsqueeze(1))\n",
    "    output_tensor = torch.cat(tensor_list, dim=1)\n",
    "    return output_tensor.float()\n",
    "\n",
    "  def _dice_loss(self, pred, target):\n",
    "    target = target.float()\n",
    "    smooth = 1e-5\n",
    "    intersect = torch.sum(pred * target)\n",
    "    y_sum = torch.sum(target * target)\n",
    "    z_sum = torch.sum(pred * pred)\n",
    "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "    loss = 1 - loss\n",
    "    return loss\n",
    "\n",
    "  def forward(self, pred, target, weight=None, softmax=False):\n",
    "    if softmax: pred = torch.softmax(pred, dim=1)\n",
    "    target = self._one_hot_encoder(target)\n",
    "    if weight is None: weight = [1] * self.n_classes\n",
    "    class_wise_dice = []\n",
    "    loss = 0\n",
    "    for i in range(self.n_classes):\n",
    "      dice = self._dice_loss(pred[:, i], target[:, i])\n",
    "      class_wise_dice.append(1 - dice.item())\n",
    "      loss += dice * weight[i]\n",
    "    return loss / self.n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, embedding_dim, head_num):\n",
    "    super().__init__()\n",
    "    self.head_num = head_num\n",
    "    self.dk = (embedding_dim // head_num) ** 0.5\n",
    "    self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n",
    "    self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    qkv = self.qkv_layer(x)\n",
    "    query, key, value = tuple(rearrange(qkv, 'b t (d k h) -> k b h t d ', k=3, h=self.head_num))\n",
    "\n",
    "    energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) * self.dk\n",
    "    if mask is not None: energy = energy.masked_fill(mask, -np.inf)\n",
    "\n",
    "    attention = torch.softmax(energy, dim=-1)\n",
    "    x = torch.einsum(\"... i j , ... j d -> ... i d\", attention, value)\n",
    "\n",
    "    x = rearrange(x, \"b h t d -> b t (h d)\")\n",
    "    x = self.out_attention(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, embedding_dim, mlp_dim):\n",
    "    super().__init__()\n",
    "    self.mlp_layers = nn.Sequential(\n",
    "      nn.Linear(embedding_dim, mlp_dim),\n",
    "      nn.GELU(),\n",
    "      nn.Dropout(0.1),\n",
    "      nn.Linear(mlp_dim, embedding_dim),\n",
    "      nn.Dropout(0.1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.mlp_layers(x)\n",
    "    return x\n",
    "  \n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "  def __init__(self, embedding_dim, head_num, mlp_dim):\n",
    "    super().__init__()\n",
    "    self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n",
    "    self.mlp = MLP(embedding_dim, mlp_dim)\n",
    "    self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "    self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    _x = self.multi_head_attention(x)\n",
    "    _x = self.dropout(_x)\n",
    "    x = x + _x\n",
    "    x = self.layer_norm1(x)\n",
    "\n",
    "    _x = self.mlp(x)\n",
    "    x = x + _x\n",
    "    x = self.layer_norm2(x)\n",
    "\n",
    "    return x\n",
    "  \n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, embedding_dim, head_num, mlp_dim, block_num=12):\n",
    "    super().__init__()\n",
    "    self.layer_blocks = nn.ModuleList(\n",
    "      [EncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)]\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer_block in self.layer_blocks:\n",
    "      x = layer_block(x)\n",
    "    return x\n",
    "  \n",
    "\n",
    "class ViT(nn.Module):\n",
    "  def __init__(\n",
    "    self, image_dim, in_channels, embedding_dim, head_num, mlp_dim,\n",
    "    block_num, patch_dim, classification=True, num_classes=1\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.patch_dim = patch_dim\n",
    "    self.classification = classification\n",
    "    self.num_tokens = (image_dim // patch_dim) ** 2\n",
    "    self.token_dim = in_channels * (patch_dim ** 2)\n",
    "\n",
    "    self.projection = nn.Linear(self.token_dim, embedding_dim)\n",
    "    self.embedding = nn.Parameter(torch.rand(self.num_tokens + 1, embedding_dim))\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    self.transformer = TransformerEncoder(embedding_dim, head_num, mlp_dim, block_num)\n",
    "\n",
    "    if self.classification:\n",
    "      self.mlp_head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    img_patches = rearrange(\n",
    "      x, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "      patch_x=self.patch_dim, patch_y=self.patch_dim\n",
    "    )\n",
    "    batch_size, tokens, _ = img_patches.shape\n",
    "\n",
    "    project = self.projection(img_patches)\n",
    "    token = repeat(self.cls_token, 'b ... -> (b batch_size) ...', batch_size=batch_size)\n",
    "\n",
    "    patches = torch.cat([token, project], dim=1)\n",
    "    patches += self.embedding[:tokens + 1, :]\n",
    "\n",
    "    x = self.dropout(patches)\n",
    "    x = self.transformer(x)\n",
    "    x = self.mlp_head(x[:, 0, :]) if self.classification else x[:, 1:, :]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. TransUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBottleneck(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, base_width=64):\n",
    "    super().__init__()\n",
    "    self.downsample = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "      nn.BatchNorm2d(out_channels)\n",
    "    )\n",
    "    width = int(out_channels * (base_width / 64))\n",
    "    self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, bias=False)\n",
    "    self.norm1 = nn.BatchNorm2d(width)\n",
    "    self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=2, groups=1, padding=1, dilation=1, bias=False)\n",
    "    self.norm2 = nn.BatchNorm2d(width)\n",
    "    self.conv3 = nn.Conv2d(width, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "    self.norm3 = nn.BatchNorm2d(out_channels)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_down = self.downsample(x)\n",
    "    x = self.conv1(x)\n",
    "    x = self.norm1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.norm2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.norm3(x)\n",
    "    x = x + x_down\n",
    "    x = self.relu(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, image_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n",
    "    self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n",
    "    self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n",
    "    self.vit_image_dim = image_dim // patch_dim\n",
    "    self.vit = ViT(\n",
    "      self.vit_image_dim, out_channels * 8, out_channels * 8,\n",
    "      head_num, mlp_dim, block_num, patch_dim=1, classification=False\n",
    "    )\n",
    "    self.conv2 = nn.Conv2d(out_channels * 8, 512, kernel_size=3, stride=1, padding=1)\n",
    "    self.norm2 = nn.BatchNorm2d(512)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.norm1(x)\n",
    "    x1 = self.relu(x)\n",
    "    x2 = self.encoder1(x1)\n",
    "    x3 = self.encoder2(x2)\n",
    "    x = self.encoder3(x3)\n",
    "    x = self.vit(x)\n",
    "    x = rearrange(x, \"b (x y) c -> b c x y\", x=self.vit_image_dim, y=self.vit_image_dim)\n",
    "    x = self.conv2(x)\n",
    "    x = self.norm2(x)\n",
    "    x = self.relu(x)\n",
    "    return x, x1, x2, x3\n",
    "  \n",
    "\n",
    "class DecoderBottleneck(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, scale_factor=2):\n",
    "    super().__init__()\n",
    "    self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)\n",
    "    self.layer = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "  def forward(self, x, x_concat=None):\n",
    "    x = self.upsample(x)\n",
    "    if x_concat is not None:\n",
    "      x = torch.cat([x_concat, x], dim=1)\n",
    "    x = self.layer(x)\n",
    "    return x\n",
    "  \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, out_channels, class_num):\n",
    "    super().__init__()\n",
    "    self.decoder1 = DecoderBottleneck(out_channels * 8, out_channels * 2)\n",
    "    self.decoder2 = DecoderBottleneck(out_channels * 4, out_channels)\n",
    "    self.decoder3 = DecoderBottleneck(out_channels * 2, int(out_channels * 1 / 2))\n",
    "    self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 8))\n",
    "    self.conv1 = nn.Conv2d(int(out_channels * 1 / 8), class_num, kernel_size=1)\n",
    "\n",
    "  def forward(self, x, x1, x2, x3):\n",
    "    x = self.decoder1(x, x3)\n",
    "    x = self.decoder2(x, x2)\n",
    "    x = self.decoder3(x, x1)\n",
    "    x = self.decoder4(x)\n",
    "    x = self.conv1(x)\n",
    "    return x\n",
    "  \n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "  def __init__(self, image_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim, class_num):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(image_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim)\n",
    "    self.decoder = Decoder(out_channels, class_num)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x, x1, x2, x3 = self.encoder(x)\n",
    "    x = self.decoder(x, x1, x2, x3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Model manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "  def __init__(self, args):\n",
    "    self.args = args\n",
    "    self.model = TransUNet(\n",
    "      args.image_dim, args.in_channels, args.out_channels, args.head_num,\n",
    "      args.mlp_dim, args.block_num, args.patch_dim, args.class_num\n",
    "    )\n",
    "    self.model = nn.DataParallel(self.model)\n",
    "    self.model.to(args.device)\n",
    "\n",
    "    self.dice_loss = DiceLoss(self.args.class_num)\n",
    "    self.ce_loss = CrossEntropyLoss()\n",
    "    self.optimizer = SGD(\n",
    "      self.model.parameters(), lr=args.learning_rate,\n",
    "      momentum=args.momentum, weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "  def load_model(self):\n",
    "    ckpt = torch.load(self.args.pretrain_path, map_location=torch.device(self.args.device), weights_only=True)\n",
    "    self.model.load_state_dict(ckpt['model_state_dict'])\n",
    "    self.optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    print(f'Checkpoint is loaded - epoc: {ckpt[\"epoch\"]} loss: {ckpt[\"loss\"]}')\n",
    "    return ckpt['epoch'], ckpt['loss']\n",
    "\n",
    "  def train_step(self, image, mask):\n",
    "    self.model.train()\n",
    "    self.optimizer.zero_grad()\n",
    "    pred_mask = self.model(image)\n",
    "    loss_ce = self.ce_loss(pred_mask, mask[:].long())\n",
    "    loss_dice = self.dice_loss(pred_mask, mask, softmax=True)\n",
    "    loss = (loss_ce + loss_dice) / 2\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item(), pred_mask\n",
    "\n",
    "  def test_step(self, image, mask):\n",
    "    self.model.eval()\n",
    "    pred_mask = self.model(image)\n",
    "    loss_ce = self.ce_loss(pred_mask, mask[:].long())\n",
    "    loss_dice = self.dice_loss(pred_mask, mask, softmax=True)\n",
    "    loss = (loss_ce + loss_dice) / 2\n",
    "    return loss.item(), pred_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochCallback:\n",
    "  end_training = False\n",
    "  not_improved_epoch = 0\n",
    "\n",
    "  def __init__(self, save_path, epochs, model, optimizer, monitor=None, patience=None, init_loss=np.inf):\n",
    "    self.save_path = save_path\n",
    "    self.epochs = epochs\n",
    "    self.monitor = monitor\n",
    "    self.patience = patience\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.monitor_value = init_loss\n",
    "\n",
    "  def __save_model(self, epoch, loss):\n",
    "    torch.save({\n",
    "      'epoch': epoch,\n",
    "      'loss': loss,\n",
    "      'model_state_dict': self.model.state_dict(),\n",
    "      'optimizer_state_dict': self.optimizer.state_dict()\n",
    "    }, self.save_path)\n",
    "    print(f'Model saved to {self.save_path}')\n",
    "\n",
    "  def epoch_end(self, epoch_num, hash):\n",
    "    epoch_end_str = f'Epoch {epoch_num}/{self.epochs} - '\n",
    "    for name, value in hash.items():\n",
    "      epoch_end_str += f'{name}: {round(value, 4)} '\n",
    "    print(epoch_end_str)\n",
    "\n",
    "    if self.monitor is None:\n",
    "      self.__save_model(epoch_num, hash[self.monitor])\n",
    "    elif hash[self.monitor] < self.monitor_value:\n",
    "      print(f'{self.monitor} decreased from {round(self.monitor_value, 4)} to {round(hash[self.monitor], 4)}')\n",
    "      self.not_improved_epoch = 0\n",
    "      self.monitor_value = hash[self.monitor]\n",
    "      self.__save_model(epoch_num, hash[self.monitor])\n",
    "    else:\n",
    "      print(f'{self.monitor} did not decrease from {round(self.monitor_value, 4)}, model did not save!')\n",
    "      self.not_improved_epoch += 1\n",
    "      if self.patience is not None and self.not_improved_epoch >= self.patience:\n",
    "        print(\"Training was stopped by callback!\")\n",
    "        self.end_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, args):\n",
    "    self.args = args\n",
    "    self.train_loader = self.__load_dataset(self.args.train_path, 'train')\n",
    "    self.test_loader = self.__load_dataset(self.args.test_path, 'test')\n",
    "    self.model_manager = ModelManager(args)\n",
    "    if (self.args.pretrain_path):\n",
    "      self.init_epoch, self.init_loss = self.model_manager.load_model()\n",
    "    else:\n",
    "      self.init_epoch, self.init_loss = 0, np.inf\n",
    "\n",
    "  def __load_dataset(self, path, split):\n",
    "    shuffle = split == 'train'\n",
    "    transform = [RandomGenerator(), Zoomer(self.args.image_dim)] if split == 'train' else [Zoomer(self.args.image_dim)]\n",
    "    transform = transforms.Compose(transform)\n",
    "    dataset = datasets[self.args.dataset_name](path, transform)\n",
    "    loader = DataLoader(dataset, batch_size=self.args.batch_size, shuffle=shuffle, num_workers=4)\n",
    "    return loader\n",
    "  \n",
    "  def __loop(self, loader, step_function, t):\n",
    "    total_loss = 0\n",
    "    for _, data in enumerate(loader):\n",
    "      image = data['image'].to(self.args.device)\n",
    "      mask = data['mask'].to(self.args.device)\n",
    "      loss, _ = step_function(image=image, mask=mask)\n",
    "      total_loss += loss\n",
    "      t.update()\n",
    "    return total_loss\n",
    "\n",
    "  def train(self):\n",
    "    callback = EpochCallback(\n",
    "      save_path=self.args.save_path, epochs=self.args.epochs,\n",
    "      model=self.model_manager.model, optimizer=self.model_manager.optimizer,\n",
    "      monitor='test_loss', patience=self.args.patience, init_loss=self.init_loss\n",
    "    )\n",
    "\n",
    "    for epoch in range(self.init_epoch, self.args.epochs):\n",
    "      with tqdm(total=len(self.train_loader) + len(self.test_loader)) as t:\n",
    "        train_loss = self.__loop(self.train_loader, self.model_manager.train_step, t)\n",
    "        test_loss = self.__loop(self.test_loader, self.model_manager.test_step, t)\n",
    "\n",
    "      callback.epoch_end(epoch + 1, {\n",
    "        'loss': train_loss / len(self.train_loader),\n",
    "        'test_loss': test_loss / len(self.test_loader)\n",
    "      })\n",
    "      if callback.end_training: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "  def __init__(self, args):\n",
    "    self.args = args\n",
    "    self.model_manager = ModelManager(args)\n",
    "    self.model_manager.load_model()\n",
    "\n",
    "  def __read_and_preprocess(self):\n",
    "    image = cv2.imread(self.args.image_path)\n",
    "    image_torch = cv2.resize(image, (self.args.image_dim, self.args.image_dim))\n",
    "    image_torch = image_torch / 255.\n",
    "    image_torch = image_torch.transpose((2, 0, 1))\n",
    "    image_torch = np.expand_dims(image_torch, axis=0)\n",
    "    image_torch = torch.from_numpy(image_torch.astype('float32')).to(self.device)\n",
    "    return image, image_torch\n",
    "  \n",
    "  def __save(self, mask):\n",
    "    cv2.imshow(\"Mask\", mask)\n",
    "\n",
    "  def __threshold(self, mask, thresh=0.5):\n",
    "    mask[mask >= thresh] = 1\n",
    "    mask[mask < thresh] = 0\n",
    "    return mask\n",
    "\n",
    "  def infer(self):\n",
    "    image, image_torch = self.__read_and_preprocess(self)\n",
    "    with torch.no_grad():\n",
    "      pred_mask = self.transunet.model(image_torch)\n",
    "      pred_mask = torch.sigmoid(pred_mask)\n",
    "      pred_mask = pred_mask.detach().cpu().numpy().transpose((0, 2, 3, 1))\n",
    "    \n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    pred_mask = cv2.resize(pred_mask[0, ...], (orig_w, orig_h))\n",
    "    pred_mask = self.__threshold(pred_mask, thresh=self.args.infer_threshold)\n",
    "    pred_mask *= 255\n",
    "\n",
    "    if self.args.merge_infer:\n",
    "      pred_mask = cv2.bitwise_and(image, image, mask=pred_mask.astype('uint8'))\n",
    "    if self.args.save_infer:\n",
    "      self.__save(pred_mask)\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Main entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "  'kaggle.ipynb',\n",
    "  '--mode', 'train',\n",
    "  '--dataset_name', 'Synapse',\n",
    "  '--class_num', '10',\n",
    "  '--train_path', '/kaggle/input/synapse/train',\n",
    "  '--test_path', '/kaggle/input/synapse/val',\n",
    "  '--save_path', '/kaggle/working/checkpoint.pth',\n",
    "  '--epochs', '100',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--mode', type=str, required=True, choices=['train', 'infer'])\n",
    "parser.add_argument('--dataset_name', required='train' in sys.argv, type=str, choices=['Synapse'])\n",
    "parser.add_argument('--train_path', required='train' in sys.argv,  type=str, default=None)\n",
    "parser.add_argument('--test_path', required='train' in sys.argv, type=str, default=None)\n",
    "parser.add_argument('--save_path', required='train' in sys.argv, type=str, default=None)\n",
    "parser.add_argument('--pretrain_path', required='infer' in sys.argv, type=str, default=None)\n",
    "parser.add_argument('--image_path', required='infer' in sys.argv, type=str, default=None)\n",
    "parser.add_argument('--merge_infer', type=bool, default=False)\n",
    "parser.add_argument('--save_infer', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=200)\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-2)\n",
    "parser.add_argument('--momentum', type=float, default=0.9)\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4)\n",
    "parser.add_argument('--patience', type=int, default=25)\n",
    "parser.add_argument('--inference_threshold', type=float, default=0.75)\n",
    "\n",
    "parser.add_argument('--image_dim', type=int, default=512)\n",
    "parser.add_argument('--in_channels', type=int, default=1)\n",
    "parser.add_argument('--out_channels', type=int, default=128)\n",
    "parser.add_argument('--head_num', type=int, default=4)\n",
    "parser.add_argument('--mlp_dim', type=int, default=512)\n",
    "parser.add_argument('--block_num', type=int, default=12)\n",
    "parser.add_argument('--patch_dim', type=int, default=16)\n",
    "parser.add_argument('--class_num', type=int, default=1)\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if args.mode == 'train':\n",
    "  trainer = Trainer(args)\n",
    "  trainer.train()\n",
    "elif args.mode == 'infer':\n",
    "  inference = Inference(args)\n",
    "  inference.infer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
